---
title: "Rick and Morty"
author: "DS Spr24 Group 6"
date: "2024-02-06"
output: html_document
---

## Initial Setup
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Load libraries
install.packages("RWeka")
install.packages("ggsci")


install.packages("magick", verbose=TRUE)

library(readr)
library(tidyverse)
install.packages("tm")
library(tm)
library(wordcloud)
library(wordcloud2)
install.packages("tidytext")
library(tidytext)
library(textdata)
library(reshape2)
library(RWeka)
library(knitr)
library(gridExtra)
library(grid)
library(magick)
library(igraph)
library(ggraph)
library("ggsci")
library(circlize)
library(radarchart)
```

## Import Data

```{r}


# Read the data 
scripts <- read_csv("RickAndMortyScripts.xls")

# Read the Lexicons (for sentiment classification)
bing <- read_csv("Bing.xls")
nrc <- read_csv("NRC.xls")
afinn <- read_csv("Afinn.xls")

# Rename Columns
scripts = scripts %>% rename(Index = "index",
                   Season.No = "season no.",
                   Episode.No = "episode no.",
                   Episode.Name = "episode name",
                   Character.Name = "name",
                   Dialog = "line")


```

## Including Plots


```{r, echo = TRUE}
s1lines = nrow(scripts[scripts$Season.No == 1,]) #getting the number of lines in each season
s2lines = nrow(scripts[scripts$Season.No == 2,])
s3lines = nrow(scripts[scripts$Season.No == 3,])

#building dataframe for plotting
lines = list(s1lines, s2lines, s3lines)
seasons = list("season 1", "Season 2", "Season 3")

linesdf <- data.frame(unlist(lines), unlist(seasons))
df = as.data.frame(linesdf)
names(df) = c("Lines", "Season Number")

#plotting number of lines in each season
p<-ggplot(data=scripts, aes(x=`Season.No`, y=nrow(scripts))) + geom_bar(stat="identity") + ggtitle("Lines per Season") + xlab("Season #") + ylab("# of Lines")
```

```{r}

#cleaning lines, pulled from https://www.kaggle.com/code/andradaolteanu/sentiment-analysis-rick-and-morty-scripts/notebook
cleanCorpus <- function(text){
  # punctuation, whitespace, lowercase, numbers
  text.tmp <- tm_map(text, removePunctuation)
  text.tmp <- tm_map(text.tmp, stripWhitespace)
  text.tmp <- tm_map(text.tmp, content_transformer(tolower))
  text.tmp <- tm_map(text.tmp, removeNumbers)
  
  # removes stopwords
  stopwords_remove <- c(stopwords("en"), c("thats","weve","hes","theres","ive","im",
                                                "will","can","cant","dont","youve","us",
                                                "youre","youll","theyre","whats","didnt"))
  text.tmp <- tm_map(text.tmp, removeWords, stopwords_remove)

  return(text.tmp)
}


```




```{r, echo=TRUE}
#getting all lines from each season
scripts1 = scripts[scripts$Season.No == 1,]
scripts2 = scripts[scripts$Season.No == 2,]
scripts3 = scripts[scripts$Season.No == 3,]

#tokenizing each season - "Look I'm a pickle"  -> "I'm" and a are removed from previous step, tokens = {look, pickle}
tokens1 <- scripts1 %>% 
  mutate(dialogue = as.character(scripts1$Dialog)) %>% 
  unnest_tokens(word, dialogue)

tokens2 <- scripts2 %>% 
  mutate(dialogue = as.character(scripts2$Dialog)) %>% 
  unnest_tokens(word, dialogue)

tokens3 <- scripts3 %>% 
  mutate(dialogue = as.character(scripts3$Dialog)) %>% 
  unnest_tokens(word, dialogue)

tokens1 %>% 
  # Count how many word per value
  inner_join(afinn, "word") %>% 
  count(value, sort=T) %>%
  
  # Plot
  ggplot(aes(x=value, y=n)) +
  geom_bar(stat="identity", aes(fill=n), show.legend = F, width = 0.5) +
  geom_label(aes(label=n)) +
  scale_fill_gradient(low="#85C1E9", high="#3498DB") +
  scale_x_continuous(breaks=seq(-5, 5, 1)) +
  labs(x="Score", y="Frequency", title="Season 1Word count distribution over intensity of sentiment: Neg -> Pos") +
  theme_bw()

tokens2 %>% 
  # Count how many word per value
  inner_join(afinn, "word") %>% 
  count(value, sort=T) %>%
  
  # Plot
  ggplot(aes(x=value, y=n)) +
  geom_bar(stat="identity", aes(fill=n), show.legend = F, width = 0.5) +
  geom_label(aes(label=n)) +
  scale_fill_gradient(low="#85C1E9", high="#3498DB") +
  scale_x_continuous(breaks=seq(-5, 5, 1)) +
  labs(x="Score", y="Frequency", title="Season 2 Word count distribution over intensity of sentiment: Neg -> Pos") +
  theme_bw()

tokens3 %>% 
  # Count how many word per value
  inner_join(afinn, "word") %>% 
  count(value, sort=T) %>%
  
  # Plot
  ggplot(aes(x=value, y=n)) +
  geom_bar(stat="identity", aes(fill=n), show.legend = F, width = 0.5) +
  geom_label(aes(label=n)) +
  scale_fill_gradient(low="#85C1E9", high="#3498DB") +
  scale_x_continuous(breaks=seq(-5, 5, 1)) +
  labs(x="Score", y="Frequency", title="Season 3 Word count distribution over intensity of sentiment: Neg -> Pos") +
  theme_bw()
```

```{r, echo=TRUE}
#assigning afinn scores to each season
affin_tok1 <- tokens1 %>% 
  inner_join(afinn, "word")

affin_tok2 <- tokens2 %>% 
  inner_join(afinn, "word") 

affin_tok3 <- tokens3 %>% 
  inner_join(afinn, "word")

#building dataframe of afinn for each season for plotting
episodes <- c(1,2,3,4,5,6,7,8,9,10)

season1 = c()
season2 = c()
season3 = c()
for (x in 1:10) {
  m1 = mean(affin_tok1[affin_tok1$Episode.No == x,]$value) #storing the mean afinn value of a season in m1
  season1 = c(season1, m1)                                 #adding m1 to the list that will be added to the dataframe
}

for (x in 1:10) {
  m2 = mean(affin_tok2[affin_tok2$Episode.No == x,]$value)
  season2 = c(season2, m2)
}

for (x in 1:10) {
  m3 = mean(affin_tok3[affin_tok3$Episode.No == x,]$value)
  season3 = c(season3, m3)
}

epdf <- data.frame(episodes, season1, season2, season3)
#dataframe
print(epdf)

```

```{r}
rm(list=ls())
```


Import Data
```{r}

dialogue <- read.csv("RickAndMortyScripts.xls")

bing <- read.csv("Bing.xls")

afinn <- read.csv("Afinn.xls")

nrc <- read.csv("NRC.xls")

```

Define a function to clean the text:
```{r}

cleanCorpus <- function(text){
  # deal with punctuation, whitespace, lowercase, numbers
  text.tmp <- tm_map(text, removePunctuation)
  text.tmp <- tm_map(text.tmp, stripWhitespace)
  text.tmp <- tm_map(text.tmp, content_transformer(tolower))
  text.tmp <- tm_map(text.tmp, removeNumbers)
  
  # removes stopwords
  stopwords_remove <- c(stopwords("en"), c("thats","weve","hes","theres","ive","im", "will","can","cant","dont","youve","us","youre","youll","theyre","whats","didnt"))
  text.tmp <- tm_map(text.tmp, removeWords, stopwords_remove)

  return(text.tmp)
}

```

## Exploratory Data Analysis

Which characters talk the most?
```{r}
# Top 10 characters with the most dialogues
dialogue %>% 
  # prepare the table
  count(name) %>%
  arrange(desc(n)) %>% 
  slice(1:10) %>%
  
  # the plot
  ggplot(aes(x=reorder(name, n), y=n)) +
  geom_bar(stat="identity", aes(fill=n), show.legend=F) +
  geom_label(aes(label=n)) +
  scale_fill_gradient(low="#58D68D", high="#239B56") +
  labs(x="Character", y="Number of dialogues", title="Most talkative in Rick And Morty") +
  coord_flip() +
  theme_bw()
```

What words are spoken most frequently?
```{r}
# Splitting up the individual tokens (words and punctuation marks)
tokens <- dialogue %>% 
  mutate(lines = as.character(dialogue$line)) %>% 
  unnest_tokens(word, lines)

# Check the output - yep, looks good!
tokens %>% head(5) %>% select(name, word)

# Count the number of times each word is spoken
tokens_count <- tokens %>% 
  # append the bing list bc this will take out the non-sentiment words that are in tokens that aren't in bing, like "I" or "it". we don't care about those
  #inner_join(bing, "word") %>%
  count(word, sort = TRUE)

# plot wordcloud of top 100 words
wordcloud(tokens_count$word[1:100], tokens_count$n[1:100])

# Remove neutral words and THEN count the number of times each word is spoken
tokens_count <- tokens %>% 
  # append the bing list bc this will take out the non-sentiment words that are in tokens that aren't in bing, like "I" or "it". we don't care about those
  inner_join(bing, "word") %>%
  count(word, sort = TRUE)

# now plot wordcloud of top words that contain *sentiment*
wordcloud(tokens_count$word, tokens_count$n)
```

Get average Afinn score for each season

```{r}

afinn_tok <- tokens %>% 
  # get afinn score for each word
  inner_join(afinn, "word")
  # select season 1 lines

s1 <- afinn_tok[afinn_tok$season.no. == 1, ]
s2 <- afinn_tok[afinn_tok$season.no. == 2, ]
s3 <- afinn_tok[afinn_tok$season.no. == 3, ]

s1_mean <- mean(s1$value)
s2_mean <- mean(s2$value)
s3_mean <- mean(s3$value)

s1_sd <- sd(s1$value)
s2_sd <- sd(s2$value)
s3_sd <- sd(s3$value)

means <- c(s1_mean, s2_mean, s3_mean)
sds <- c(s1_sd, s2_sd, s3_sd)

# compile the mean and std dev data into data table
season_sentiment_data <- data.frame(season = c(1,2,3), mean = means, std_dev = sds)

# plot the mean sentiments for each season
p<-ggplot(data=season_sentiment_data, aes(x=season, y=mean)) +
  geom_bar(stat="identity") +
  labs( x = "Season", y = "Mean Sentiment Score")
p

```

Do ANOVA hypothesis testing on season means

```{r}

# then perform ANOVA
one.way <- aov(mean ~ season, data = season_sentiment_data)
summary(one.way)

```

The p value of the one-way ANOVA is 0.534 which is larger than the alpha of 0.05. We do not find a significantly significant difference between the average sentiment of each season. Therefore, we reject our hypothesis that Rick and Morty's average season sentiment gets more negative as time goes on.
